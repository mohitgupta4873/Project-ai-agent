{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "tyWVW4eAI0Rz"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U langgraph google-generativeai pydantic~=2.7\n",
        "\n",
        "import os\n",
        "import textwrap\n",
        "from datetime import datetime\n",
        "from typing import TypedDict, Optional, Literal\n",
        "\n",
        "import google.generativeai as genai\n",
        "from langgraph.graph import StateGraph, END\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: Paste your key (quick for Colab)\n",
        "GEMINI_API_KEY = input(\"Enter your Gemini API key:\").strip()\n",
        "\n",
        "# Option B: If you prefer env var, set it once and skip input:\n",
        "# os.environ[\"GEMINI_API_KEY\"] = \"YOUR_KEY_HERE\"\n",
        "# GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "assert GEMINI_API_KEY, \"Gemini API key is required.\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Use a fast + capable model; swap to 'gemini-1.5-pro' for deeper reasoning.\n",
        "GEMINI_MODEL_NAME = \"gemini-1.5-flash\"\n",
        "model = genai.GenerativeModel(GEMINI_MODEL_NAME)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R52RSlxuI59r",
        "outputId": "df068df6-3fd8-4584-f84f-0e61874a0d20"
      },
      "execution_count": 48,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Gemini API key:AIzaSyD--J6K3LcDq05GWyKinXMHve51ioltggk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HiringState(TypedDict, total=False):\n",
        "    # Recruiter inputs\n",
        "    role_request: str                     # e.g., \"Backend Engineer, 0–2 years\"\n",
        "    requirements: Optional[str]           # optional bullet points or comma list\n",
        "\n",
        "    # LLM outputs and loop info\n",
        "    jd_draft: Optional[str]               # latest JD draft in Markdown\n",
        "    feedback: Optional[str]               # recruiter feedback/instructions\n",
        "    status: Literal[\"INIT\", \"DRAFTED\", \"NEEDS_CHANGES\", \"APPROVED\"]\n",
        "\n",
        "def make_jd_prompt(state: HiringState) -> str:\n",
        "    \"\"\"Build a structured prompt for Gemini to generate a high-quality JD.\"\"\"\n",
        "    role = state.get(\"role_request\", \"\").strip()\n",
        "    reqs = state.get(\"requirements\", \"\")\n",
        "    feedback = (state.get(\"feedback\") or \"\").strip()\n",
        "\n",
        "    base_instructions = f\"\"\"\n",
        "    You are an expert technical recruiter. Create a clear, inclusive, and concise Job Description in **Markdown**.\n",
        "\n",
        "    ROLE: {role or \"[MISSING]\"}\n",
        "    KNOWN REQUIREMENTS (if any): {reqs or \"—\"}\n",
        "\n",
        "    OUTPUT FORMAT (Markdown headings):\n",
        "    # Job Title\n",
        "    ## About the Role\n",
        "    ## Responsibilities\n",
        "    ## Must-Have Qualifications\n",
        "    ## Good-to-Have Qualifications\n",
        "    ## Tech Stack\n",
        "    ## Impact & Growth\n",
        "    ## Compensation & Benefits\n",
        "    ## Location & Work Setup\n",
        "    ## Interview Process\n",
        "    ## How to Apply\n",
        "\n",
        "    STYLE:\n",
        "    - Inclusive, jargon-light, outcome-focused, use nice styling, bold the headlines wherever required.\n",
        "    - Bullet points where it helps readability.\n",
        "    - Avoid bias or age/college prestige signals; focus on skills/impact.\n",
        "    - Keep it strictly 1100 characters.\n",
        "    - Dont be too monotonic and machinic in tone, be a little quirky, dont make such g=rigid and structured jd, instead make something eye-catching and worth attention\n",
        "\n",
        "    NO PLACEHOLDERS WHATSOEVER:\n",
        "    - the jd shouldnt have any unknown piece of information,assume anything that u find missing\n",
        "\n",
        "    \"\"\"\n",
        "    feedback_block = f\"\\nRECRUITER FEEDBACK TO INCORPORATE:\\n{feedback}\\n\" if feedback else \"\"\n",
        "    return textwrap.dedent(base_instructions + feedback_block).strip()\n"
      ],
      "metadata": {
        "id": "U-pqOoDzJOqm"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def capture_request(state: HiringState) -> HiringState:\n",
        "    \"\"\"Prompt for recruiter input only if missing (keeps this node reusable).\"\"\"\n",
        "    role_request = state.get(\"role_request\")\n",
        "    requirements = state.get(\"requirements\")\n",
        "\n",
        "    if not role_request:\n",
        "        role_request = input(\"Enter role (e.g., 'Backend Engineer, 0–2 years'): \").strip()\n",
        "\n",
        "    if requirements is None:\n",
        "        print(\"\\n(Optional) Paste any key requirements (or leave blank):\")\n",
        "        requirements = input().strip() or None\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"role_request\": role_request,\n",
        "        \"requirements\": requirements,\n",
        "        \"status\": \"INIT\",\n",
        "    }\n",
        "\n",
        "def generate_jd(state: HiringState) -> HiringState:\n",
        "    \"\"\"Call Gemini to generate a JD draft using the current state.\"\"\"\n",
        "    prompt = make_jd_prompt(state)\n",
        "    try:\n",
        "        resp = model.generate_content(prompt)\n",
        "        jd_text = (resp.text or \"\").strip()\n",
        "        if not jd_text:\n",
        "            raise ValueError(\"Empty JD draft from model.\")\n",
        "    except Exception as e:\n",
        "        jd_text = f\"ERROR: Failed to generate JD: {e}\"\n",
        "\n",
        "    print(\"\\n\" + \"#\"*30 + \" JD DRAFT \" + \"#\"*30 + \"\\n\")\n",
        "    print(jd_text)\n",
        "    print(\"\\n\" + \"#\"*74 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"jd_draft\": jd_text,\n",
        "        \"status\": \"DRAFTED\",\n",
        "    }\n",
        "\n",
        "def get_feedback(state: HiringState) -> HiringState:\n",
        "    \"\"\"\n",
        "    Show options:\n",
        "      (A)pprove → end\n",
        "      (R)egenerate → loop back (optionally ask 'what should be different?')\n",
        "      (E)dit → take freeform edit instructions and regenerate\n",
        "    \"\"\"\n",
        "    print(\"Actions: [A]pprove  |  [R]egenerate  |  [E]dit with instructions\")\n",
        "    choice = input(\"Your choice (A/R/E): \").strip().lower()\n",
        "\n",
        "    if choice in (\"a\", \"approve\"):\n",
        "        print(\"\\n✅ Approved. Saving JD…\")\n",
        "        return {**state, \"status\": \"APPROVED\", \"feedback\": None}\n",
        "\n",
        "    if choice in (\"r\", \"regen\", \"regenerate\"):\n",
        "        why = input(\"What should be different (tone, scope, stack, level, brevity, etc.)? Leave blank for a fresh take: \").strip()\n",
        "        fb = f\"Regenerate with these adjustments: {why}\" if why else \"Regenerate with a different approach; vary structure/tone while keeping the role.\"\n",
        "        return {**state, \"status\": \"NEEDS_CHANGES\", \"feedback\": fb}\n",
        "\n",
        "    if choice in (\"e\", \"edit\"):\n",
        "        fb = input(\"Paste precise edit instructions (add/remove bullets, change qualifications, tech, etc.): \").strip()\n",
        "        if not fb:\n",
        "            fb = \"Apply general improvements for clarity, concision, and inclusivity.\"\n",
        "        return {**state, \"status\": \"NEEDS_CHANGES\", \"feedback\": fb}\n",
        "\n",
        "    print(\"Didn't catch that—defaulting to regenerate with general improvements.\")\n",
        "    return {**state, \"status\": \"NEEDS_CHANGES\", \"feedback\": \"General improvements and alternative phrasing.\"}\n"
      ],
      "metadata": {
        "id": "5LXhIaxfJcUS"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = StateGraph(HiringState)\n",
        "\n",
        "graph.add_node(\"capture_request\", capture_request)\n",
        "graph.add_node(\"generate_jd\", generate_jd)\n",
        "graph.add_node(\"get_feedback\", get_feedback)\n",
        "\n",
        "# Flow: capture → generate → feedback → (approved → END) or (needs changes → generate)\n",
        "graph.set_entry_point(\"capture_request\")\n",
        "graph.add_edge(\"capture_request\", \"generate_jd\")\n",
        "graph.add_edge(\"generate_jd\", \"get_feedback\")\n",
        "\n",
        "def route_after_feedback(state: HiringState):\n",
        "    return \"END\" if state.get(\"status\") == \"APPROVED\" else \"generate_jd\"\n",
        "\n",
        "graph.add_conditional_edges(\"get_feedback\", route_after_feedback, {\"generate_jd\": \"generate_jd\", \"END\": END})\n",
        "\n",
        "app = graph.compile()\n"
      ],
      "metadata": {
        "id": "fvPDI02UJff9"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Start lean and type everything interactively\n",
        "final_state = app.invoke({})\n",
        "\n",
        "# Option 2: Preseed with a role and optional requirements to speed up\n",
        "# seed = {\n",
        "#     \"role_request\": \"Backend Engineer, 0–2 years\",\n",
        "#     \"requirements\": \"Python, FastAPI, PostgreSQL, Docker; familiarity with AWS; strong CS fundamentals\",\n",
        "# }\n",
        "# final_state = app.invoke(seed)\n",
        "\n",
        "# Persist the final JD if approved\n",
        "if final_state.get(\"status\") == \"APPROVED\" and final_state.get(\"jd_draft\"):\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "    fname = f\"JD_{final_state.get('role_request','role').replace(' ','_')}_{ts}.md\"\n",
        "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(final_state[\"jd_draft\"])\n",
        "    print(f\"\\n💾 Saved: {fname}\")\n",
        "else:\n",
        "    print(\"\\nℹ️ Not approved yet. Rerun the last cell to continue iterating if needed.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mAWGs4t1Jkt4",
        "outputId": "bfa91ad2-f5e8-4a7d-d6bf-5db73a5033aa"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter role (e.g., 'Backend Engineer, 0–2 years'): ai developer\n",
            "\n",
            "(Optional) Paste any key requirements (or leave blank):\n",
            "\n",
            "\n",
            "############################## JD DRAFT ##############################\n",
            "\n",
            "# **AI Developer:  Shape the Future!**\n",
            "\n",
            "## **About the Role**\n",
            "\n",
            "Join our dynamic team building cutting-edge AI solutions. We're looking for a passionate and skilled AI Developer to contribute to innovative projects that make a real-world impact.  We value creativity and collaboration,  fostering an environment where everyone thrives.\n",
            "\n",
            "## **Responsibilities**\n",
            "\n",
            "* Design, develop, and deploy AI models.\n",
            "* Collaborate with cross-functional teams.\n",
            "* Improve existing AI systems.\n",
            "* Stay current with the latest AI advancements.\n",
            "\n",
            "## **Must-Have Qualifications**\n",
            "\n",
            "* Proficiency in Python and relevant AI/ML libraries (e.g., TensorFlow, PyTorch).\n",
            "* Experience building and deploying machine learning models.\n",
            "* Strong problem-solving and analytical skills.\n",
            "* Excellent communication skills.\n",
            "\n",
            "\n",
            "## **Good-to-Have Qualifications**\n",
            "\n",
            "* Experience with cloud platforms (AWS, GCP, Azure).\n",
            "* Familiarity with data visualization tools.\n",
            "* Experience in a relevant field.\n",
            "\n",
            "\n",
            "## **Tech Stack**\n",
            "\n",
            "Python, TensorFlow, PyTorch,  AWS/GCP/Azure (one or more)\n",
            "\n",
            "\n",
            "## **Impact & Growth**\n",
            "\n",
            "Your work will directly influence our product's success. We provide ample opportunities for skill development and career advancement.\n",
            "\n",
            "## **Compensation & Benefits**\n",
            "\n",
            "Competitive salary and comprehensive benefits package including health insurance, paid time off, and professional development opportunities.\n",
            "\n",
            "## **Location & Work Setup**\n",
            "\n",
            "Remote or hybrid options available.\n",
            "\n",
            "\n",
            "## **Interview Process**\n",
            "\n",
            "Initial screening, technical assessment, and final interview with the team.\n",
            "\n",
            "## **How to Apply**\n",
            "\n",
            "Submit your resume and cover letter highlighting relevant experience to [email protected]\n",
            "\n",
            "##########################################################################\n",
            "\n",
            "Actions: [A]pprove  |  [R]egenerate  |  [E]dit with instructions\n",
            "Your choice (A/R/E): A\n",
            "\n",
            "✅ Approved. Saving JD…\n",
            "\n",
            "💾 Saved: JD_ai_developer_20250824_1506.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langgraph requests\n",
        "import requests, json\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Optional"
      ],
      "metadata": {
        "id": "-xGbSTI_UXtC"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HiringState(TypedDict, total=False):\n",
        "    role_request: str\n",
        "    jd_draft: Optional[str]\n",
        "    status: str\n",
        "\n",
        "    # Step 2\n",
        "    google_form_link: Optional[str]\n",
        "    linkedin_access_token: Optional[str]\n",
        "    linkedin_author_urn: Optional[str]\n",
        "    linkedin_post_id: Optional[str]\n",
        "    linkedin_post_url: Optional[str]\n"
      ],
      "metadata": {
        "id": "4uSDjpgnUaBB"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_form(state: HiringState) -> HiringState:\n",
        "    form_link = input(\"Paste your Google Form link (already created): \").strip()\n",
        "    return {**state, \"google_form_link\": form_link}"
      ],
      "metadata": {
        "id": "EncV6so7UcGL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_linkedin(state: HiringState) -> HiringState:\n",
        "    jd = state.get(\"jd_draft\", \"\")\n",
        "    role = state.get(\"role_request\", \"Role\")\n",
        "    form_link = state.get(\"google_form_link\", \"\")\n",
        "\n",
        "    # Recruiter inputs\n",
        "    access_token = input(\"Enter your LinkedIn Access Token: \").strip()\n",
        "    author_urn = input(\"Enter your LinkedIn Author URN (e.g., urn:li:person:xxxx): \").strip()\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"X-Restli-Protocol-Version\": \"2.0.0\"\n",
        "    }\n",
        "\n",
        "    # Post text: job summary + form link\n",
        "    # LinkedIn has a character limit for posts. We'll include the role, form link,\n",
        "    # and as much of the JD as possible within the limit.\n",
        "    max_post_length = 1100 # LinkedIn post character limit is around 1300\n",
        "    initial_post_text = f\"🚀 We're hiring: {role}\\n\\nApply here: {form_link}\\n \\n\"\n",
        "    remaining_length = max_post_length - len(initial_post_text)\n",
        "\n",
        "    # Truncate the JD to fit within the remaining length, adding an ellipsis\n",
        "    jd_to_include = jd\n",
        "    if len(jd_to_include) > remaining_length:\n",
        "        jd_to_include = jd[:remaining_length - 4] + \"...\" # -4 for ellipsis\n",
        "\n",
        "    post_text = initial_post_text + jd_to_include\n",
        "\n",
        "\n",
        "    payload = {\n",
        "        \"author\": author_urn,\n",
        "        \"lifecycleState\": \"PUBLISHED\",\n",
        "        \"specificContent\": {\n",
        "            \"com.linkedin.ugc.ShareContent\": {\n",
        "                \"shareCommentary\": { \"text\": post_text },\n",
        "                \"shareMediaCategory\": \"NONE\"\n",
        "            }\n",
        "        },\n",
        "        \"visibility\": { \"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\" }\n",
        "    }\n",
        "\n",
        "    resp = requests.post(\"https://api.linkedin.com/v2/ugcPosts\", headers=headers, data=json.dumps(payload))\n",
        "\n",
        "    if resp.status_code in (200,201):\n",
        "        try:\n",
        "            data = resp.json()\n",
        "            post_id = data.get(\"id\")\n",
        "            post_url = f\"https://www.linkedin.com/feed/update/{post_id}\" if post_id else None\n",
        "            print(f\"\\n✅ Posted on LinkedIn: {post_url}\")\n",
        "            return {\n",
        "                **state,\n",
        "                \"linkedin_access_token\": access_token,\n",
        "                \"linkedin_author_urn\": author_urn,\n",
        "                \"linkedin_post_id\": post_id,\n",
        "                \"linkedin_post_url\": post_url\n",
        "            }\n",
        "        except Exception:\n",
        "            print(\"\\n⚠️ Posted, but response parse failed. Raw response:\")\n",
        "            print(resp.text)\n",
        "            return state\n",
        "    else:\n",
        "        print(f\"\\n❌ Failed to post: {resp.status_code}, {resp.text}\")\n",
        "        return state"
      ],
      "metadata": {
        "id": "Ph8cPJLxUhJg"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph2 = StateGraph(HiringState)\n",
        "graph2.add_node(\"create_form\", create_form)\n",
        "graph2.add_node(\"post_linkedin\", post_linkedin)\n",
        "\n",
        "graph2.set_entry_point(\"create_form\")\n",
        "graph2.add_edge(\"create_form\", \"post_linkedin\")\n",
        "\n",
        "app2 = graph2.compile()\n"
      ],
      "metadata": {
        "id": "z2wGZIhIUjj1"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume Step 1 is done and JD is approved\n",
        "# Load the JD draft from the saved file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Find the latest saved JD file\n",
        "jd_files = glob.glob(\"JD_*.md\")\n",
        "if jd_files:\n",
        "    latest_jd_file = max(jd_files, key=os.path.getctime)\n",
        "    with open(latest_jd_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        jd_draft = f.read()\n",
        "    print(f\"Loaded JD draft from: {latest_jd_file}\")\n",
        "else:\n",
        "    jd_draft = None\n",
        "    print(\"No JD draft file found. Please run the first graph execution to generate one.\")\n",
        "\n",
        "\n",
        "seed_state = {\n",
        "    \"role_request\": \"\", # Replace with the actual role request if needed\n",
        "    \"jd_draft\": jd_draft,\n",
        "    \"status\": \"APPROVED\" # Assuming the loaded JD is approved\n",
        "}\n",
        "\n",
        "if jd_draft:\n",
        "    final_state2 = app2.invoke(seed_state)\n",
        "\n",
        "    print(\"\\n📌 Final State:\")\n",
        "    print(final_state2)\n",
        "else:\n",
        "    print(\"Cannot proceed with the second graph execution without a JD draft.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP5EQHScUmCB",
        "outputId": "e7a0e57c-c1e0-4eeb-80c6-ddb0ceaa265c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded JD draft from: JD_ai_developer_20250824_1506.md\n",
            "Paste your Google Form link (already created): https://docs.google.com/forms/d/1301GqoxqYjpy4dSrcMn26dXBbRvyueaGawNUfhfdWg0\n",
            "Enter your LinkedIn Access Token: AQVoPMBgu6z8LyAU5UQu9FFZs2jckFf6l7rHXKYINRC_aS38aypLKySbSuz6Vfsjsbcjr8Dd8YY-dFEoHFesq2QW01N28Q5szB7qKPbcS_j5vZNL7x2etx7LbaeOkUIE1o73I0PT77vt2HP-Cc87VPsiPoDeYjNQvt7tM-yNXPm2NSKQNoywSmRF3tfB9R6D7792PhrwoUmgMhnW430563vtFmhZmK-OFa1WmHF0Yj-9GFtz4hY_-0qNUPvm4ol_iPl5ZtDsmH0Jleb7oRLoknnoyBjth0vJlqCMEP4wmPuPCha3raxepdf9A5jdaajoigobXrw1kZF4xwdO3tRjAixa5vOmXA\n",
            "Enter your LinkedIn Author URN (e.g., urn:li:person:xxxx): urn:li:person:pUP3keeY_z\n",
            "\n",
            "✅ Posted on LinkedIn: https://www.linkedin.com/feed/update/urn:li:share:7365399329005625344\n",
            "\n",
            "📌 Final State:\n",
            "{'role_request': '', 'jd_draft': \"# **AI Developer:  Shape the Future!**\\n\\n## **About the Role**\\n\\nJoin our dynamic team building cutting-edge AI solutions. We're looking for a passionate and skilled AI Developer to contribute to innovative projects that make a real-world impact.  We value creativity and collaboration,  fostering an environment where everyone thrives.\\n\\n## **Responsibilities**\\n\\n* Design, develop, and deploy AI models.\\n* Collaborate with cross-functional teams.\\n* Improve existing AI systems.\\n* Stay current with the latest AI advancements.\\n\\n## **Must-Have Qualifications**\\n\\n* Proficiency in Python and relevant AI/ML libraries (e.g., TensorFlow, PyTorch).\\n* Experience building and deploying machine learning models.\\n* Strong problem-solving and analytical skills.\\n* Excellent communication skills.\\n\\n\\n## **Good-to-Have Qualifications**\\n\\n* Experience with cloud platforms (AWS, GCP, Azure).\\n* Familiarity with data visualization tools.\\n* Experience in a relevant field.\\n\\n\\n## **Tech Stack**\\n\\nPython, TensorFlow, PyTorch,  AWS/GCP/Azure (one or more)\\n\\n\\n## **Impact & Growth**\\n\\nYour work will directly influence our product's success. We provide ample opportunities for skill development and career advancement.\\n\\n## **Compensation & Benefits**\\n\\nCompetitive salary and comprehensive benefits package including health insurance, paid time off, and professional development opportunities.\\n\\n## **Location & Work Setup**\\n\\nRemote or hybrid options available.\\n\\n\\n## **Interview Process**\\n\\nInitial screening, technical assessment, and final interview with the team.\\n\\n## **How to Apply**\\n\\nSubmit your resume and cover letter highlighting relevant experience to [email protected]\", 'status': 'APPROVED', 'google_form_link': 'https://docs.google.com/forms/d/1301GqoxqYjpy4dSrcMn26dXBbRvyueaGawNUfhfdWg0', 'linkedin_access_token': 'AQVoPMBgu6z8LyAU5UQu9FFZs2jckFf6l7rHXKYINRC_aS38aypLKySbSuz6Vfsjsbcjr8Dd8YY-dFEoHFesq2QW01N28Q5szB7qKPbcS_j5vZNL7x2etx7LbaeOkUIE1o73I0PT77vt2HP-Cc87VPsiPoDeYjNQvt7tM-yNXPm2NSKQNoywSmRF3tfB9R6D7792PhrwoUmgMhnW430563vtFmhZmK-OFa1WmHF0Yj-9GFtz4hY_-0qNUPvm4ol_iPl5ZtDsmH0Jleb7oRLoknnoyBjth0vJlqCMEP4wmPuPCha3raxepdf9A5jdaajoigobXrw1kZF4xwdO3tRjAixa5vOmXA', 'linkedin_author_urn': 'urn:li:person:pUP3keeY_z', 'linkedin_post_id': 'urn:li:share:7365399329005625344', 'linkedin_post_url': 'https://www.linkedin.com/feed/update/urn:li:share:7365399329005625344'}\n"
          ]
        }
      ]
    }
  ]
}